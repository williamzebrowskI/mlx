{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('../data/wiki.test.txt', 'r') as f:\n",
    "    vocab = f.read()\n",
    "\n",
    "sample = vocab[:100]\n",
    "len(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "save_dir = \"../data/book.txt\"\n",
    "\n",
    "\n",
    "def load_data(save_dir):\n",
    "    # *NB* First file is expected to be the training set\n",
    "    with open(save_dir, \"r\") as fid:\n",
    "        vocab = set(fid.read().split())\n",
    "    eos = \"<eos>\"\n",
    "    unk = \"<unk>\"  # Add unknown token\n",
    "    vocab.add(eos)\n",
    "    vocab.add(unk)\n",
    "    vocab = {v: i for i, v in enumerate(vocab)}\n",
    "    return vocab\n",
    "\n",
    "load_data(save_dir)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentencepiece as spm\n",
    "\n",
    "# Define training parameters\n",
    "input_file = \"../data/book.txt\"\n",
    "model_prefix = \"llm\"\n",
    "vocab_size = 30000\n",
    "character_coverage = 1.0  # 100% coverage for most languages\n",
    "\n",
    "# Train SentencePiece model using BPE\n",
    "spm.SentencePieceTrainer.train(\n",
    "    input=input_file,\n",
    "    model_prefix=model_prefix,\n",
    "    vocab_size=vocab_size,\n",
    "    model_type='bpe',\n",
    "    character_coverage=character_coverage,\n",
    "    user_defined_symbols=['<PAD>', '<UNK>', '<CLS>', '<SEP>']  # Add special tokens\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[271,\n",
       " 791,\n",
       " 473,\n",
       " 5241,\n",
       " 818,\n",
       " 17723,\n",
       " 569,\n",
       " 574,\n",
       " 28016,\n",
       " 2391,\n",
       " 264,\n",
       " 4261,\n",
       " 315,\n",
       " 2294,\n",
       " 96710,\n",
       " 22510,\n",
       " 304,\n",
       " 13527,\n",
       " 1174,\n",
       " 13160,\n",
       " 555,\n",
       " 279,\n",
       " 18678,\n",
       " 297]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tiktoken\n",
    "enc = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "for i in \n",
    "\n",
    "enc.encode(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
