{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2623c877",
   "metadata": {},
   "source": [
    "\n",
    "# Deep Dive into Neural Networks: Understanding Forward and Backward Propagation with MLX\n",
    "\n",
    "This notebook is designed to help you achieve an expert-level understanding of neural networks, particularly focusing on the detailed calculations that occur during the forward and backward propagation processes. We will explore each step involved in these processes, breaking down complex operations into understandable parts.\n",
    "\n",
    "## What You Will Learn\n",
    "- **How a Neural Network Processes Inputs**: Understand the transformation from input to output in the forward pass.\n",
    "- **How Gradients are Computed and Used**: Dive deep into the backpropagation algorithm to see how the network learns.\n",
    "- **Detailed Mathematical Breakdown**: Manual calculations for each step to solidify your understanding.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8652f530",
   "metadata": {},
   "source": [
    "\n",
    "# Understanding Neural Networks with MLX\n",
    "In this notebook, we will explore how a simple neural network works using the MLX framework. We will walk through the process of:\n",
    "- Initializing the network with weights and biases.\n",
    "- Performing a forward pass to compute predictions.\n",
    "- Calculating the loss.\n",
    "- Performing backpropagation to compute gradients.\n",
    "- Updating the weights and biases to reduce the loss.\n",
    "\n",
    "Let's get started!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a618733a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import mlx.core as mx\n",
    "import mlx.nn as nn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3984d174",
   "metadata": {},
   "source": [
    "\n",
    "## Defining the Neural Network\n",
    "We will define a simple neural network with one hidden layer. The network will have the following architecture:\n",
    "- Input layer: Takes in the input features.\n",
    "- Hidden layer: Fully connected layer with a ReLU activation function.\n",
    "- Output layer: Fully connected layer that outputs the prediction.\n",
    "\n",
    "We will also define the weights and biases manually for clarity.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff270259",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Manually initialize weights and biases for clarity\n",
    "        self.weights1 = mx.random.uniform(-1, 1, (input_dim, hidden_dim))  # Weights from input to hidden layer\n",
    "        self.bias1 = mx.zeros((hidden_dim,))  # Bias for hidden layer\n",
    "\n",
    "        self.weights2 = mx.random.uniform(-1, 1, (hidden_dim, output_dim))  # Weights from hidden to output layer\n",
    "        self.bias2 = mx.zeros((output_dim,))  # Bias for output layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Forward pass: Compute hidden layer activations\n",
    "        z1 = mx.matmul(x, self.weights1) + self.bias1\n",
    "        a1 = mx.maximum(z1, 0)  # ReLU activation\n",
    "\n",
    "        # Forward pass: Compute output layer activations\n",
    "        z2 = mx.matmul(a1, self.weights2) + self.bias2\n",
    "        return z2  # No activation function in output (e.g., for regression)\n",
    "\n",
    "# Initialize the network\n",
    "input_dim = 2  # Number of input features\n",
    "hidden_dim = 3  # Number of neurons in the hidden layer\n",
    "output_dim = 1  # Number of output neurons\n",
    "\n",
    "model = SimpleNN(input_dim, hidden_dim, output_dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "57a639de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([[-0.0746283, -0.750631, 0.238348],\n",
      "       [-0.233977, -0.885689, -0.267174]], dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9ab857c",
   "metadata": {},
   "source": [
    "\n",
    "## Performing the Forward Pass\n",
    "Next, we'll define a simple input and perform the forward pass through the network. We'll manually calculate the activations at each layer to understand how the network processes inputs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "63e3b926",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output of the network: array([[0]], dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define a simple input (e.g., a batch of 1 sample with 2 features)\n",
    "x = mx.array([[1.0, 2.0]])\n",
    "\n",
    "# Perform the forward pass\n",
    "output = model.forward(x)\n",
    "print(\"Output of the network:\", output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d82310cd",
   "metadata": {},
   "source": [
    "\n",
    "## Manual Calculation of the Forward Pass\n",
    "Let's manually calculate the activations step by step to see how the input moves through the network.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6609ef38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hidden layer activations (z1): array([[-0.542583, -2.52201, -0.296]], dtype=float32)\n",
      "Hidden layer activations after ReLU (a1): array([[0, 0, 0]], dtype=float32)\n",
      "Output layer activations (z2): array([[0]], dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Hidden layer calculation\n",
    "z1_manual = mx.matmul(x, model.weights1) + model.bias1\n",
    "a1_manual = mx.maximum(z1_manual, 0)  # ReLU activation\n",
    "\n",
    "# Output layer calculation\n",
    "z2_manual = mx.matmul(a1_manual, model.weights2) + model.bias2\n",
    "\n",
    "print(\"Hidden layer activations (z1):\", z1_manual)\n",
    "print(\"Hidden layer activations after ReLU (a1):\", a1_manual)\n",
    "print(\"Output layer activations (z2):\", z2_manual)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "419646c3",
   "metadata": {},
   "source": [
    "\n",
    "## Defining a Loss Function\n",
    "We will use a simple Mean Squared Error (MSE) loss function to measure the difference between the network's predictions and the true target values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "937cc3a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: array(9, dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define a simple target output for the loss calculation\n",
    "y_true = mx.array([[3.0]])\n",
    "\n",
    "# Define the MSE loss function\n",
    "def mse_loss(y_pred, y_true):\n",
    "    return mx.mean(mx.square(y_pred - y_true))\n",
    "\n",
    "# Calculate the loss\n",
    "loss = mse_loss(output, y_true)\n",
    "print(\"Loss:\", loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b19b1d87",
   "metadata": {},
   "source": [
    "\n",
    "## Backpropagation and Gradient Calculation\n",
    "Now, we'll perform backpropagation to compute the gradients of the loss with respect to the network's weights and biases. These gradients will tell us how to adjust the parameters to reduce the loss.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eb47ef98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradients for weights1: array([[0, 0, 0],\n",
      "       [0, 0, 0]], dtype=float32)\n",
      "Gradients for bias1: array([0, -0, -0], dtype=float32)\n",
      "Gradients for weights2: array([[0],\n",
      "       [0],\n",
      "       [0]], dtype=float32)\n",
      "Gradients for bias2: array([-6], dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define the function that computes loss and gradients\n",
    "def loss_and_grad_fn(x, y_true):\n",
    "    y_pred = model.forward(x)  # Forward pass\n",
    "    loss = mse_loss(y_pred, y_true)  # Loss calculation\n",
    "    return loss\n",
    "\n",
    "# Compute loss and gradients using nn.value_and_grad\n",
    "loss, grads = nn.value_and_grad(model, loss_and_grad_fn)(x, y_true)\n",
    "\n",
    "# Extract gradients for weights and biases\n",
    "grads_w1 = grads[\"weights1\"]\n",
    "grads_b1 = grads[\"bias1\"]\n",
    "grads_w2 = grads[\"weights2\"]\n",
    "grads_b2 = grads[\"bias2\"]\n",
    "\n",
    "print(\"Gradients for weights1:\", grads_w1)\n",
    "print(\"Gradients for bias1:\", grads_b1)\n",
    "print(\"Gradients for weights2:\", grads_w2)\n",
    "print(\"Gradients for bias2:\", grads_b2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "622bc556",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
